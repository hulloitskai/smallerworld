# typed: true

# DO NOT EDIT MANUALLY
# This is an autogenerated file for types exported from the `words_counted` gem.
# Please instead update this file by running `bin/tapioca gem words_counted`.


# source://words_counted//lib/refinements/hash_refinements.rb#2
module Refinements; end

# source://words_counted//lib/refinements/hash_refinements.rb#3
module Refinements::HashRefinements; end

# source://words_counted//lib/words_counted/deprecated.rb#2
module WordsCounted
  class << self
    # Takes a string, tokenises it, and returns an instance of Counter
    # with the resulting tokens.
    #
    # @param input [String] The input to be tokenised
    # @param options [Hash] The options to pass onto `Counter`
    # @return [WordsCounted::Counter] An instance of Counter
    # @see Tokeniser.tokenise
    # @see Counter.initialize
    #
    # source://words_counted//lib/words_counted.rb#25
    def count(input, options = T.unsafe(nil)); end

    # Takes a file path, reads the file and tokenises its contents,
    # and returns an instance of Counter with the resulting tokens.
    #
    # @param path [String] The file to be read and tokenised
    # @param options [Hash] The options to pass onto `Counter`
    # @return [WordsCounted::Counter] An instance of Counter
    # @see Tokeniser.tokenise
    # @see Counter.initialize
    #
    # source://words_counted//lib/words_counted.rb#39
    def from_file(path, options = T.unsafe(nil)); end
  end
end

# source://words_counted//lib/words_counted/counter.rb#5
class WordsCounted::Counter
  include ::WordsCounted::Deprecated

  # Initializes state with an array of tokens.
  #
  # @param An [Array] array of tokens to perform operations on
  # @return [Counter] a new instance of Counter
  #
  # source://words_counted//lib/words_counted/counter.rb#21
  def initialize(tokens); end

  # Returns the average char count per token rounded to a precision of two decimal places.
  # Accepts a `precision` argument.
  #
  # @example
  #   Counter.new(%w[one three five seven]).average_chars_per_token
  #   # => 4.25
  # @param precision [Integer] The number of decimal places to round average char count to
  # @return [Float] The average char count per token
  #
  # source://words_counted//lib/words_counted/counter.rb#133
  def average_chars_per_token(precision: T.unsafe(nil)); end

  # Returns the character count of all tokens.
  #
  # @example
  #   Counter.new(%w[one two]).char_count
  #   # => 6
  # @return [Integer] The total char count of tokens
  #
  # source://words_counted//lib/words_counted/counter.rb#54
  def char_count; end

  # Returns a hash of tokens and their lengths for tokens with the highest length
  #
  # @example
  #   Counter.new(%w[one three five seven]).longest_tokens
  #   # => { 'three' => 5, 'seven' => 5 }
  # @return [Hash{String => Integer}] A hash of tokens and their lengths
  #
  # source://words_counted//lib/words_counted/counter.rb#120
  def longest_tokens; end

  # Returns a hash of tokens and their frequencies for tokens with the highest frequency.
  #
  # @example
  #   Counter.new(%w[one once two two twice twice]).most_frequent_tokens
  #   # => { 'two' => 2, 'twice' => 2 }
  # @return [Hash{String => Integer}] A hash of tokens and their frequencies
  #
  # source://words_counted//lib/words_counted/counter.rb#109
  def most_frequent_tokens; end

  # Returns the number of tokens.
  #
  # @example
  #   Counter.new(%w[one two two three three three]).token_count
  #   # => 6
  # @return [Integer] The number of tokens
  #
  # source://words_counted//lib/words_counted/counter.rb#32
  def token_count; end

  # Returns a sorted two-dimensional array where each member array is a token and its density
  # as a float, rounded to a precision of two decimal places. It accepts a precision argument
  # which defaults to `2`.
  #
  # @example
  #   Counter.new(%w[Maj. Major Major Major]).token_density
  #   # => [ ['major', .75], ['maj', .25] ]
  # @example with `precision`
  #   Counter.new(%w[Maj. Major Major Major]).token_density(precision: 4)
  #   # => [ ['major', .7500], ['maj', .2500] ]
  # @param precision [Integer] The number of decimal places to round density to
  # @return [Array<Array<String, Float>>] An array of tokens and their densities
  #
  # source://words_counted//lib/words_counted/counter.rb#96
  def token_density(precision: T.unsafe(nil)); end

  # Returns a sorted two-dimensional array where each member array is a token and its frequency.
  # The array is sorted by frequency in descending order.
  #
  # @example
  #   Counter.new(%w[one two two three three three]).token_frequency
  #   # => [ ['three', 3], ['two', 2], ['one', 1] ]
  # @return [Array<Array<String, Integer>>] An array of tokens and their frequencies
  #
  # source://words_counted//lib/words_counted/counter.rb#66
  def token_frequency; end

  # Returns a sorted two-dimensional array where each member array is a token and its length.
  # The array is sorted by length in descending order.
  #
  # @example
  #   Counter.new(%w[one two three four five]).token_lenghts
  #   # => [ ['three', 5], ['four', 4], ['five', 4], ['one', 3], ['two', 3] ]
  # @return [Array<Array<String, Integer>>] An array of tokens and their lengths
  #
  # source://words_counted//lib/words_counted/counter.rb#78
  def token_lengths; end

  # @return [Array<String>] an array of tokens.
  #
  # source://words_counted//lib/words_counted/counter.rb#16
  def tokens; end

  # Returns the number of unique tokens.
  #
  # @example
  #   Counter.new(%w[one two two three three three]).uniq_token_count
  #   # => 3
  # @return [Integer] The number of unique tokens
  #
  # source://words_counted//lib/words_counted/counter.rb#43
  def uniq_token_count; end
end

# source://words_counted//lib/words_counted/deprecated.rb#3
module WordsCounted::Deprecated
  # @deprecated use `Counter#average_chars_per_token`
  #
  # source://words_counted//lib/words_counted/deprecated.rb#67
  def average_chars_per_word(precision = T.unsafe(nil)); end

  # @deprecated use `Counter#average_chars_per_token`
  #
  # source://words_counted//lib/words_counted/deprecated.rb#73
  def count(token); end

  # @deprecated use `Counter#longest_tokens`
  #
  # source://words_counted//lib/words_counted/deprecated.rb#60
  def longest_words; end

  # @deprecated use `Counter#most_frequent_tokens`
  #
  # source://words_counted//lib/words_counted/deprecated.rb#53
  def most_occurring_words; end

  # @deprecated use `Counter#token_lengths`
  #
  # source://words_counted//lib/words_counted/deprecated.rb#47
  def sorted_word_lengths; end

  # @deprecated use `Counter#token_frequency`
  #
  # source://words_counted//lib/words_counted/deprecated.rb#41
  def sorted_word_occurrences; end

  # @deprecated use `Counter#uniq_token_count`
  #
  # source://words_counted//lib/words_counted/deprecated.rb#13
  def unique_word_count; end

  # @deprecated use `Counter#token_count`
  #
  # source://words_counted//lib/words_counted/deprecated.rb#7
  def word_count; end

  # @deprecated use `Counter#token_density`
  #
  # source://words_counted//lib/words_counted/deprecated.rb#33
  def word_density(precision = T.unsafe(nil)); end

  # @deprecated use `Counter#token_lengths`
  #
  # source://words_counted//lib/words_counted/deprecated.rb#26
  def word_lengths; end

  # @deprecated use `Counter#token_frequency`
  #
  # source://words_counted//lib/words_counted/deprecated.rb#19
  def word_occurrences; end
end

# source://words_counted//lib/words_counted/tokeniser.rb#3
class WordsCounted::Tokeniser
  # Initialises state with the string to be tokenised.
  #
  # @param input [String] The string to tokenise
  # @return [Tokeniser] a new instance of Tokeniser
  #
  # source://words_counted//lib/words_counted/tokeniser.rb#21
  def initialize(input); end

  # Converts a string into an array of tokens using a regular expression.
  # If a regexp is not provided a default one is used. See `Tokenizer.TOKEN_REGEXP`.
  #
  # Use `exclude` to remove tokens from the final list. `exclude` can be a string,
  # a regular expression, a lambda, a symbol, or an array of one or more of those types.
  # This allows for powerful and flexible tokenisation strategies.
  #
  # If a symbol is passed, it must name a predicate method.
  #
  # @example
  #   WordsCounted::Tokeniser.new("Hello World").tokenise
  #   # => ['hello', 'world']
  # @example With `pattern`
  #   WordsCounted::Tokeniser.new("Hello-Mohamad").tokenise(pattern: /[^-]+/)
  #   # => ['hello', 'mohamad']
  # @example With `exclude` as a string
  #   WordsCounted::Tokeniser.new("Hello Sami").tokenise(exclude: "hello")
  #   # => ['sami']
  # @example With `exclude` as a regexp
  #   WordsCounted::Tokeniser.new("Hello Dani").tokenise(exclude: /hello/i)
  #   # => ['dani']
  # @example With `exclude` as a lambda
  #   WordsCounted::Tokeniser.new("Goodbye Sami").tokenise(
  #   exclude: ->(token) { token.length > 6 }
  #   )
  #   # => ['sami']
  # @example With `exclude` as a symbol
  #   WordsCounted::Tokeniser.new("Hello محمد").tokenise(exclude: :ascii_only?)
  #   # => ['محمد']
  # @example With `exclude` as an array of strings
  #   WordsCounted::Tokeniser.new("Goodbye Sami and hello Dani").tokenise(
  #   exclude: ["goodbye hello"]
  #   )
  #   # => ['sami', 'and', dani']
  # @example With `exclude` as an array of regular expressions
  #   WordsCounted::Tokeniser.new("Goodbye and hello Dani").tokenise(
  #   exclude: [/goodbye/i, /and/i]
  #   )
  #   # => ['hello', 'dani']
  # @example With `exclude` as an array of lambdas
  #   t = WordsCounted::Tokeniser.new("Special Agent 007")
  #   t.tokenise(
  #   exclude: [
  #   ->(t) { t.to_i.odd? },
  #   ->(t) { t.length > 5}
  #   ]
  #   )
  #   # => ['agent']
  # @example With `exclude` as a mixed array
  #   t = WordsCounted::Tokeniser.new("Hello! اسماءنا هي محمد، كارولينا، سامي، وداني")
  #   t.tokenise(
  #   exclude: [
  #   :ascii_only?,
  #   /محمد/,
  #   ->(t) { t.length > 6},
  #   "و"
  #   ]
  #   )
  #   # => ["هي", "سامي", "وداني"]
  # @param pattern [Regexp] The string to tokenise
  # @param exclude [Array<String, Regexp, Lambda, Symbol>, String, Regexp, Lambda, Symbol, nil] The filter to apply
  # @return [Array] The array of filtered tokens
  #
  # source://words_counted//lib/words_counted/tokeniser.rb#97
  def tokenise(pattern: T.unsafe(nil), exclude: T.unsafe(nil)); end

  private

  # @api private
  #
  # source://words_counted//lib/words_counted/tokeniser.rb#156
  def filter_proc_from_string(filter); end

  # @api private
  #
  # source://words_counted//lib/words_counted/tokeniser.rb#148
  def filter_procs_from_array(filter); end

  # @api private
  #
  # source://words_counted//lib/words_counted/tokeniser.rb#130
  def filter_to_proc(filter); end
end

# Default tokenisation strategy
#
# source://words_counted//lib/words_counted/tokeniser.rb#16
WordsCounted::Tokeniser::TOKEN_REGEXP = T.let(T.unsafe(nil), Regexp)

# source://words_counted//lib/words_counted/version.rb#3
WordsCounted::VERSION = T.let(T.unsafe(nil), String)
